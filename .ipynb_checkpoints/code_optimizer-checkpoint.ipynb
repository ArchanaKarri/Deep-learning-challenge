{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Read in the charity_data.csv file to a Pandas DataFrame\n",
    "df = pd.read_csv(\"charity_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "\n",
    "# Identify the target and feature variables\n",
    "target = df[\"IS_SUCCESSFUL\"]\n",
    "features = df.drop(columns=[\"IS_SUCCESSFUL\", \"EIN\", \"NAME\"])\n",
    "\n",
    "# Drop the EIN and NAME columns\n",
    "df = df.drop(columns=[\"EIN\", \"NAME\"])\n",
    "\n",
    "# Determine the number of unique values for each column\n",
    "unique_values = df.nunique()\n",
    "print(unique_values)\n",
    "\n",
    "# For columns with more than 10 unique values, determine the number of data points for each unique value\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() > 10:\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Pick a cutoff point to bin \"rare\" categorical variables together in a new value, \"Other\"\n",
    "cutoff = 1000\n",
    "\n",
    "# Bin \"rare\" categorical variables together in a new value, \"Other\"\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() > 10:\n",
    "        value_counts = df[col].value_counts()\n",
    "        to_replace = value_counts[value_counts <= cutoff].index\n",
    "        df[col] = df[col].replace(to_replace, \"Other\")\n",
    "\n",
    "# Use pd.get_dummies() to encode the categorical variables\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "# Split the preprocessed data into a features array and a target array\n",
    "X = df.drop(columns=[\"IS_SUCCESSFUL\"])\n",
    "y = df[\"IS_SUCCESSFUL\"]\n",
    "\n",
    "# Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Scale the training and testing features datasets\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design a neural network model\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(keras.layers.Dense(units=128, activation='relu', input_dim=X_\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
